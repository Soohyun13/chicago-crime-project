# -*- coding: utf-8 -*-
"""[DAB4] FP_T4의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wv9U5l6O_-OT4xweh2QvbuZRaacUxfHb
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

!gdown https://drive.google.com/uc?id=1Ppx7kH7cLdDB5yXaIwEl5QTVhZwoR9y3 -O ./Full_GTD_Dataset_Full_Version.xlsx

"""## 1차 칼럼 정제
1. 국제 테러에 대해 다루기 때문에 미국에 한정한 칼럼의 경우 제외 (nkillus, nwoundus, nhostkidus, ransomamtus, ransompaidus)
2. claim 관련  
3. 일부 필요 없다고 판단한 칼럼은 제외
  - compclaim: 테러 단체 간의 문제이므로 우리가 분석해야되는 데이터 대상이 아니라고 판단되어 제거
  - propvalue & propcomment: propextent, propextent_txt 컬럼과 중복되어 컬럼 제거
  - divert/kidhijcountry: 납치하여 우회한 국가에 대한 정보로, 유의미하다고 판단되지 않아 제거
  - summary/motive/weapdetail/addnotes: 데이터 컬럼에 대한 설명 컬럼이기에 제거
  - scite1/scite2/scite3/dbsource: 데이터에 대한 출처 컬럼이기에 제거
  - nkllus/nwoundus/nhostkidus/ransomamtus/ransompaidus: ~us(측정 대상이 오직 미국)인 컬럼은 모두 제거
"""

df = pd.read_excel('./Full_GTD_Dataset_Full_Version.xlsx')
df.info()

df = df.drop(columns = ['compclaim', 'propvalue', 'propcomment', 'divert', 'kidhijcountry'])
df.info()

pd.set_option('display.max_columns', None)

df.head()

df = df.drop(columns = ['summary', 'motive', 'weapdetail', 'addnotes', 'scite1', 'scite2', 'scite3', 'dbsource'])

df.head()

df = df.drop(columns = ['nkillus', 'nwoundus', 'nhostkidus', 'ransomamtus', 'ransompaidus'])
df.info()

# Commented out IPython magic to ensure Python compatibility.
# # 시각화 시 한글 깨짐 방지 위한 폰트 설치 및 적용
# 
# %%capture
# !sudo apt-get install -y fonts-nanum
# !sudo fc-cache -fv
# !rm ~/.cache/matplotlib -rf
# 
# plt.rc('font', family='NanumBarunGothic')
# plt.rcParams['axes.unicode_minus'] =False

# 주요 칼럼 선택
columns = ["gname", "attacktype1", "nkill", "nkillter", "nwound", "nwoundte", "propextent"]
df = df[columns]

# 결측치 처리
df.fillna(0, inplace=True)

# 테러 조직별 선호 공격 유형 찾기
fav_attack = df.groupby("gname")['attacktype1'].agg(lambda x: x.value_counts().idxmax())

# 피해 규모 계산
damage_metrics = df.groupby("attacktype1")[["nkill", "nkillter", "nwound", "nwoundte", "propextent"]].mean()

# 시각화
plt.figure(figsize=(12, 6))
sns.heatmap(damage_metrics, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Average Damage Metrics by Attack Type")
plt.xlabel("Damage Type")
plt.ylabel("Attack Type")
plt.show()

# 결과 출력
print("테러 조직별 선호 공격 유형:")
print(fav_attack)
print("\n공격 유형별 평균 피해 규모:")
print(damage_metrics)

columns = ["suicide", "success", "attacktype1"]
df = df[columns]

# 자살 테러 성공률 분석
suicide_success_rate = df[df['suicide'] == 1]['success'].mean()
nonsuicide_success_rate = df[df['suicide'] == 0]['success'].mean()

# 공격 유형별 자살 테러 성공률
suicide_attack_success = df[df['suicide'] == 1].groupby('attacktype1')['success'].mean()

# 시각화: 자살 vs 비자살 테러 성공률 비교
plt.figure(figsize=(6, 4))
sns.barplot(x=['Suicide Attack', 'Non-Suicide Attack'], y=[suicide_success_rate, nonsuicide_success_rate], palette=['red', 'blue'])
plt.ylabel("Success Rate")
plt.title("Success Rate of Suicide vs Non-Suicide Attacks")
plt.show()

# 시각화: 공격 유형별 자살 테러 성공률
plt.figure(figsize=(10, 5))
sns.barplot(x=suicide_attack_success.index, y=suicide_attack_success.values, palette='Reds')
plt.xticks(rotation=45)
plt.ylabel("Success Rate")
plt.xlabel("Attack Type")
plt.title("Success Rate of Suicide Attacks by Attack Type")
plt.show()

# 결과 출력
print("\n자살 테러 성공률:", suicide_success_rate)
print("비자살 테러 성공률:", nonsuicide_success_rate)
print("\n공격 유형별 자살 테러 성공률:")
print(suicide_attack_success)

df = pd.read_excel('./Full_GTD_Dataset_Full_Version.xlsx')
# 공격 유형별 성공률
attack_success_rate = df.groupby("attacktype1")["success"].mean()

# 공격 유형별 피해 수준 (사망자, 부상자, 재산 피해 평균)
attack_damage = df.groupby("attacktype1")[["nkill", "nkillter", "nwound", "nwoundte", "propextent"]].mean()

plt.figure(figsize=(10, 5))
sns.barplot(x=attack_success_rate.index, y=attack_success_rate.values, palette="Blues")
plt.xlabel("Attack Type")
plt.ylabel("Success Rate")
plt.title("Success Rate by Attack Type")
plt.xticks(rotation=45)
plt.show()

attack_damage.plot(kind="bar", figsize=(12, 6), colormap="coolwarm")
plt.xlabel("Attack Type")
plt.ylabel("Average Damage")
plt.title("Average Damage by Attack Type")
plt.xticks(rotation=45)
plt.legend(["Killed", "Terrorist Killed", "Wounded", "Terrorist Wounded"])
plt.show()

print("공격 유형별 성공률:")
print(attack_success_rate)

print("\n공격 유형별 평균 피해 수준:")
print(attack_damage)

df = pd.read_excel('./Full_GTD_Dataset_Full_Version.xlsx')

# 공격 유형 합치기 (attacktype1, attacktype2, attacktype3을 하나의 리스트로 변환)
df["combined_attack"] = df[["attacktype1", "attacktype2", "attacktype3"]].values.tolist()

# 각 테러 조직이 가장 많이 사용한 공격 유형 찾기
fav_attack = df.groupby("gname")["attacktype1"].agg(lambda x: x.value_counts().idxmax())

# 복합공격 여부 (attacktype2 또는 attacktype3이 존재하면 복합공격으로 처리)
df["multi_attack"] = df[["attacktype2", "attacktype3"]].notna().sum(axis=1) > 0

# 테러 조직별 복합공격 비율 계산
multi_attack_rate = df.groupby("gname")["multi_attack"].mean()

# 공격 유형별 평균 피해 규모 계산
damage_metrics = df.groupby("attacktype1")[["nkill", "nkillter", "nwound", "nwoundte", "propextent"]].mean()

import matplotlib.pyplot as plt
import seaborn as sns

# 복합공격 비율 시각화
plt.figure(figsize=(12, 5))
multi_attack_rate.sort_values(ascending=False).head(10).plot(kind="bar", color="orange")
plt.ylabel("Multi-Attack Rate")
plt.xlabel("Terrorist Group")
plt.title("Top 10 Terrorist Groups with Highest Multi-Attack Rate")
plt.xticks(rotation=45)
plt.show()

# 공격 유형별 피해 규모 시각화
damage_metrics.plot(kind="bar", figsize=(12, 6), colormap="coolwarm")
plt.xlabel("Attack Type")
plt.ylabel("Average Damage")
plt.title("Average Damage by Attack Type")
plt.xticks(rotation=45)
plt.legend(["Killed", "Terrorist Killed", "Wounded", "Terrorist Wounded", "Property Damage"])
plt.show()

print("테러 조직별 선호 공격 유형:")
print(fav_attack)

print("\n테러 조직별 복합공격 비율:")
print(multi_attack_rate)

print("\n공격 유형별 평균 피해 규모:")
print(damage_metrics)

df = pd.read_excel('./Full_GTD_Dataset_Full_Version.xlsx')

# 2nd, 3rd 공격 여부를 새로운 컬럼으로 변환 (있으면 1, 없으면 0)
df["has_2nd_attack"] = df["attacktype2"].notna().astype(int)
df["has_3rd_attack"] = df["attacktype3"].notna().astype(int)

# 2nd, 3rd 공격을 포함하는 복합공격 여부
df["multi_attack"] = (df["has_2nd_attack"] + df["has_3rd_attack"]) > 0

# 2nd, 3rd 공격 여부 비율 확인
second_attack_rate = df["has_2nd_attack"].mean()
third_attack_rate = df["has_3rd_attack"].mean()

print(f"2nd Attack 포함 비율: {second_attack_rate:.2%}")
print(f"3rd Attack 포함 비율: {third_attack_rate:.2%}")

# 공격 유형별 단독 vs. 복합공격 비율
attack_multi_ratio = df.groupby("attacktype1")["multi_attack"].mean()

# 시각화
plt.figure(figsize=(10, 5))
sns.barplot(x=attack_multi_ratio.index, y=attack_multi_ratio.values, palette="Blues")
plt.xlabel("Attack Type")
plt.ylabel("Multi-Attack Rate")
plt.title("Multi-Attack Rate by Attack Type")
plt.xticks(rotation=45)
plt.show()

# 단독 공격 vs. 복합 공격의 평균 피해 규모 비교
single_attack_damage = df[df["multi_attack"] == 0][["nkill", "nwound", "propextent"]].mean()
multi_attack_damage = df[df["multi_attack"] == 1][["nkill", "nwound", "propextent"]].mean()

# 데이터 정리
damage_comparison = pd.DataFrame({
    "Single Attack": single_attack_damage,
    "Multi Attack": multi_attack_damage
})

# 시각화
damage_comparison.plot(kind="bar", figsize=(8, 5), colormap="coolwarm")
plt.ylabel("Average Damage")
plt.title("Single Attack vs. Multi Attack: Damage Comparison")
plt.xticks(rotation=45)
plt.show()

print("단독 공격 평균 피해 규모:")
print(single_attack_damage)
print("\n복합 공격 평균 피해 규모:")
print(multi_attack_damage)

df = pd.read_excel('./Full_GTD_Dataset_Full_Version.xlsx')

# 'gname' 컬럼을 기준으로 테러 단체별 빈도수 계산
group_counts = df['gname'].value_counts()

# 시각화
plt.figure(figsize=(10, 6))
group_counts.plot(kind='barh', color='lightseagreen')
plt.title('Top 10 Terrorist Groups')
plt.xlabel('Number of Attacks')
plt.ylabel('Terrorist Group')
plt.gca().invert_yaxis()  # 상위 단체가 위로 오도록
plt.show()

!pip install geopandas
!wget https://naturalearth.s3.amazonaws.com/110m_cultural/ne_110m_admin_0_countries.zip
!unzip ne_110m_admin_0_countries.zip

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import imageio

# 데이터 불러오기 (파일 경로를 직접 지정해야 함)
df = pd.read_excel('./Full_GTD_Dataset_Full_Version.xlsx')

df = df.sort_values(by='iyear')

# 세계 지도 데이터 불러오기
# Changed the file path to the downloaded shapefile
world = gpd.read_file("./ne_110m_admin_0_countries.shp")

# 연도 범위 설정
years = df['iyear'].unique()

# 이미지 저장할 리스트
images = []

for year in years:
    fig, ax = plt.subplots(figsize=(10, 6))
    world.plot(ax=ax, color='lightgrey')  # 기본 세계 지도 배경

    # 해당 연도 데이터 필터링
    year_data = df[df['iyear'] == year]

    # 사건 위치 시각화
    plt.scatter(year_data['longitude'], year_data['latitude'], color='red', alpha=0.6, s=10, label=f'Year {year}')

    plt.title(f'Terrorism Events in {year}')
    plt.legend()
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')

    # 이미지 저장
    filename = f"frame_{year}.png"
    plt.savefig(filename)
    plt.close()

    images.append(imageio.imread(filename))

# GIF 생성
imageio.mimsave('terrorism_trend.gif', images, duration=0.5)

df = pd.read_excel('./Full_GTD_Dataset_Full_Version.xlsx')

import pandas as pd
import matplotlib.pyplot as plt

# 결측치 제거
df = df.dropna(subset=['targtype1_txt', 'targsubtype1_txt'])

# 타깃별 공격 횟수 집계
target_counts = df['targtype1_txt'].value_counts()

# 상위 10개 주요 타깃 선택
top_targets = target_counts.head(10).index

# 상위 10개 주요 타깃 데이터 필터링
filtered_df = df[df['targtype1_txt'].isin(top_targets)]

# 주요 타깃별 하위 유형 공격 횟수 집계 (Pivot Table)
pivot_df = filtered_df.pivot_table(index='targtype1_txt',
                                   columns='targsubtype1_txt',
                                   aggfunc='size',
                                   fill_value=0)

# 그래프 그리기
plt.figure(figsize=(14, 7))
pivot_df.plot(kind='bar', stacked=True, colormap='tab10', figsize=(14, 7))

plt.title('Top 10 Most Attacked Targets by Subtype')
plt.xlabel('Target Type')
plt.ylabel('Number of Attacks')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Subtype', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.show()

df = pd.read_excel('./Full_GTD_Dataset_Full_Version.xlsx')

import pandas as pd
import scipy.stats as stats

!pip install openpyxl
import pandas as pd

# 데이터 불러오기 (GTD 데이터셋)
df = pd.read_excel("./Full_GTD_Dataset_Full_Version.xlsx", engine='openpyxl')

# 내전 발생 지역 리스트
civil_war_countries = ["Syria", "Iraq", "Afghanistan", "Yemen", "Libya", "Somalia"]

# 내전 발생 지역 여부 컬럼 추가
df["civil_war_zone"] = df["country_txt"].apply(lambda x: 1 if x in civil_war_countries else 0)

# 테러 유형 필터링 (무장 공격: 'Armed Assault', 폭발/폭격: 'Bombing/Explosion')
target_types = ["Armed Assault", "Bombing/Explosion"]

# 내전 지역과 비내전 지역에서 테러 유형 빈도 계산
contingency_table = pd.crosstab(df["civil_war_zone"], df["attacktype1_txt"].isin(target_types))

# 카이제곱 검정 수행
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)

# 결과 출력
print("카이제곱 통계량:", chi2)
print("p-value:", p)

# p-value < 0.05이면 유의미한 차이가 있음 (즉, 내전 지역에서 해당 테러 유형 비율이 높을 가능성)
if p < 0.05:
    print("내전 지역과 비내전 지역에서 테러 유형 분포 차이가 통계적으로 유의미함.")
else:
    print("내전 지역과 비내전 지역의 테러 유형 분포 차이가 유의미하지 않음.")

import pandas as pd
import scipy.stats as stats

# 데이터 불러오기 (GTD 데이터셋)
df = pd.read_excel("./Full_GTD_Dataset_Full_Version.xlsx")

# 내전 발생 국가 리스트 (사용자가 정의해야 함)
civil_war_countries = ["Syria", "Iraq", "Afghanistan", "Yemen", "Libya", "Somalia"]

# 내전 발생 여부 컬럼 추가
df["civil_war_zone"] = df["country_txt"].apply(lambda x: 1 if x in civil_war_countries else 0)

# 국가별 테러 발생 여부 계산
country_terror_counts = df["country_txt"].value_counts().reset_index()
country_terror_counts.columns = ["country_txt", "terror_count"]
country_terror_counts["terror_occurred"] = (country_terror_counts["terror_count"] > 0).astype(int)

# 내전 여부 데이터 결합
country_terror_counts["civil_war_zone"] = country_terror_counts["country_txt"].apply(lambda x: 1 if x in civil_war_countries else 0)

# 카이제곱 검정 수행
contingency_table = pd.crosstab(country_terror_counts["civil_war_zone"], country_terror_counts["terror_occurred"])
chi2, p, dof, expected = stats.chi2_contingency(contingency_table)

# 결과 출력
print("=== 카이제곱 검정 결과 ===")
print("카이제곱 통계량:", chi2)
print("p-value:", p)
if p < 0.05:
    print("내전 여부는 테러 발생 여부에 유의미한 영향을 줌.")
else:
    print("내전 여부는 테러 발생 여부에 유의미한 영향을 주지 않음.")

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error

# 1. 전처리
mask = ((df['nperps'] == -99) | pd.isna(df['nperps'])) & (df['nperpcap'] >= 1)

# 해당 조건에 맞는 행들의 nperps 값을 nperpcap으로 업데이트
df.loc[mask, 'nperps'] = df.loc[mask, 'nperpcap']
df['nperps'] = df['nperps'].fillna(-99)
df['nperpcap'] = df['nperpcap'].fillna(-99)

# nperps가 -9인 값을 -99로 바꾸기
df['nperps'] = df['nperps'].replace(-9, -99)
df['nperpcap'] = df['nperpcap'].replace(-9, -99)

df['propextent_txt'] = df['propextent_txt'].fillna('Unknown')
df['propextent_txt'] = df['propextent_txt'].replace('7', 'Unknown')
df['propextent'] = df['propextent'].replace(7, 4)
df['propextent'] = df['propextent'].fillna(4)

# Remove rows with NaN in target variables 'nperps' and 'nperpcap' after preprocessing
df = df[df['nperps'].notna() & df['nperpcap'].notna()]

# Additionally, remove rows where 'nperps' and 'nperpcap' are -99
df = df[(df['nperps'] != -99) & (df['nperpcap'] != -99)]

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# NaN 처리
df = df.dropna(subset=['nperps', 'nperpcap'])  # 또는 df['nperps'].fillna(0, inplace=True); df['nperpcap'].fillna(0, inplace=True)

X = df[['iyear', 'attacktype1', 'attacktype2', 'attacktype3', 'propextent_txt', 'region_txt', 'attacktype1_txt']]
y_nperps = df['nperps']
y_nperpcap = df['nperpcap']

X_train, X_test, y_nperps_train, y_nperps_test, y_nperpcap_train, y_nperpcap_test = train_test_split(
    X, y_nperps, y_nperpcap, test_size=0.2, random_state=42
)

categorical_features = ['region_txt', 'attacktype1_txt', 'propextent_txt', 'attacktype1', 'attacktype2', 'attacktype3']
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ],
    remainder='passthrough'
)

models = {
    'RandomForest': RandomForestRegressor(random_state=42),
    'GradientBoosting': GradientBoostingRegressor(random_state=42)
}

results = {}

for model_name, model in models.items():
    pipeline_nperps = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])
    pipeline_nperpcap = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    pipeline_nperps.fit(X_train, y_nperps_train)
    pipeline_nperpcap.fit(X_train, y_nperpcap_train)

    pred_nperps = pipeline_nperps.predict(X_test)
    pred_nperpcap = pipeline_nperpcap.predict(X_test)

    results[model_name] = {
        'nperps': {
            'MAE': mean_absolute_error(y_nperps_test, pred_nperps),
            'RMSE': np.sqrt(mean_squared_error(y_nperps_test, pred_nperps)),
            'R2': r2_score(y_nperps_test, pred_nperps)
        },
        'nperpcap': {
            'MAE': mean_absolute_error(y_nperpcap_test, pred_nperpcap),
            'RMSE': np.sqrt(mean_squared_error(y_nperpcap_test, pred_nperpcap)),
            'R2': r2_score(y_nperpcap_test, pred_nperpcap)
        }
    }

best_model_name_nperps = max(results, key=lambda x: results[x]['nperps']['R2'])
best_model_name_nperpcap = max(results, key=lambda x: results[x]['nperpcap']['R2'])

best_model_nperps = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', models[best_model_name_nperps])
])
best_model_nperpcap = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', models[best_model_name_nperpcap])
])

best_model_nperps.fit(X, y_nperps)
best_model_nperpcap.fit(X, y_nperpcap)

print("최적 모델 재훈련 완료")

from sklearn.metrics import r2_score

# 모델이 이미 예측한 값이 pred_nperps, pred_nperpcap이라고 가정
r2_nperps = r2_score(y_nperps_test, pred_nperps)
r2_nperpcap = r2_score(y_nperpcap_test, pred_nperpcap)

print(f"nperps R²: {r2_nperps:.4f}")
print(f"nperpcap R²: {r2_nperpcap:.4f}")

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
!pip install catboost
from catboost import CatBoostRegressor


# 데이터 준비
X = df[['iyear', 'attacktype1', 'attacktype2', 'attacktype3', 'propextent_txt', 'region_txt', 'attacktype1_txt']]
y_nperps = df['nperps']
y_nperpcap = df['nperpcap']

# 학습 및 테스트 데이터 분리
X_train, X_test, y_nperps_train, y_nperps_test, y_nperpcap_train, y_nperpcap_test = train_test_split(
    X, y_nperps, y_nperpcap, test_size=0.2, random_state=42
)

# 전처리 파이프라인 설정
categorical_features = ['region_txt', 'attacktype1_txt', 'propextent_txt', 'attacktype1', 'attacktype2', 'attacktype3']
numerical_features = ['iyear']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)
    ],
    remainder='passthrough'
)

# 모델 설정
models = {
    'RandomForest': RandomForestRegressor(n_estimators=200, random_state=42, max_depth=10),
    'GradientBoosting': GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42),
    'XGBoost': XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=5, random_state=42),
    'CatBoost': CatBoostRegressor(iterations=200, learning_rate=0.1, depth=5, verbose=0, random_state=42)
}

# 모델 평가
kf = KFold(n_splits=5, shuffle=True, random_state=42)
results = {}

for model_name, model in models.items():
    print(f"\n모델 훈련 중: {model_name}")

    # nperps 모델
    pipeline_nperps = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    # nperpcap 모델
    pipeline_nperpcap = Pipeline([
        ('preprocessor', preprocessor),
        ('regressor', model)
    ])

    # --- The Fix: Impute NaN values in target variables before cross-validation ---
    y_nperps_train = np.nan_to_num(y_nperps_train) # Replace NaN with 0
    y_nperpcap_train = np.nan_to_num(y_nperpcap_train) # Replace NaN with 0
    # -----------------------------------------------------------------------------

    # 교차 검증 적용
    r2_nperps = cross_val_score(pipeline_nperps, X_train, y_nperps_train, cv=kf, scoring='r2').mean()
    r2_nperpcap = cross_val_score(pipeline_nperpcap, X_train, y_nperpcap_train, cv=kf, scoring='r2').mean()

    results[model_name] = {
        'nperps_R2': r2_nperps,
        'nperpcap_R2': r2_nperpcap
    }

    print(f"  nperps R²: {r2_nperps:.4f}")
    print(f"  nperpcap R²: {r2_nperpcap:.4f}")

# 최적 모델 선택
best_model_nperps = max(results, key=lambda x: results[x]['nperps_R2'])
best_model_nperpcap = max(results, key=lambda x: results[x]['nperpcap_R2'])

print(f"\n최적 모델:")
print(f"  nperps: {best_model_nperps}")
print(f"  nperpcap: {best_model_nperpcap}")

# 1. 데이터 불러오기
df = pd.read_excel('./Full_GTD_Dataset_Full_Version.xlsx')

# 2. 필요한 컬럼 선택 + attacktype1~3 포함
selected_cols = [
    'iyear', 'country_txt', 'region_txt',
    'attacktype1_txt', 'attacktype2_txt', 'attacktype3_txt',
    'weaptype1_txt', 'nkill', 'nwound', 'success', 'suicide'
]
df = df[selected_cols]

# 3. 결측치 처리
df['nkill'].fillna(0, inplace=True)
df['nwound'].fillna(0, inplace=True)

# 4. 새 타겟 변수 생성: 피해 규모 기준
df['casualties'] = df['nkill'] + df['nwound']
df['is_severe'] = df['casualties'].apply(lambda x: 1 if x >= 10 else 0)

# 5. 공격 유형 통합 (중복 제거 후 문자열 결합)
df['attacktype_combined'] = df[['attacktype1_txt', 'attacktype2_txt', 'attacktype3_txt']]\
    .astype(str).values.tolist()
df['attacktype_combined'] = df['attacktype_combined'].apply(lambda x: ','.join(set([i for i in x if i != 'nan'])))

# → 예: "Bombing/Explosion,Armed Assault"

# 6. 범주형 변수 인코딩
label_cols = ['country_txt', 'region_txt', 'attacktype_combined', 'weaptype1_txt']
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
for col in label_cols:
    df[col] = le.fit_transform(df[col])

# 7. 학습/테스트셋 분리
X = df[['iyear', 'country_txt', 'region_txt', 'attacktype_combined', 'weaptype1_txt', 'success', 'suicide']]
y = df['is_severe']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 8. 모델 학습 및 변수 중요도
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# 9. 중요 변수 시각화
import matplotlib.pyplot as plt
import seaborn as sns

importances = rf.feature_importances_
feature_names = X.columns
feature_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
feature_imp_df.sort_values(by='Importance', ascending=False, inplace=True)

sns.barplot(x='Importance', y='Feature', data=feature_imp_df)
plt.title('Feature Importance in Predicting Severe Terror Attacks')
plt.show()